{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "import seaborn as sns\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, p_conn = 10, 750, 0.05 # very sparse, as 0.01 gives us on average every GC connects to 1 MC.\n",
    "max_lim = 0.5 # for each entry Wnm the max connection strength\n",
    "\n",
    "W = utils.create_network(M, N, p_conn, max_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(W.T.sum(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(W,cmap = 'flare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = np.linspace(0.01, 1, M)\n",
    "odor1 = [x**2 for x in list1]\n",
    "odor2 = [x**2 for x in list1[::-1]]\n",
    "odors = np.array([odor1, odor2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_responpse = get_GCact(W, odors.T, theta = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(odors, cmap = 'Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(GC_responpse !=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(GC_responpse.T, cmap = 'Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## inner loop\n",
    "\n",
    "### Newton's method; it works!\n",
    "\n",
    "Will do both that and line search; but perhaps line search is cheaper\n",
    "\n",
    "Need to check negative why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "## it'd be wise to leave W out, since in the inner loop it's not supposed to be changed.\n",
    "\n",
    "def get_err(GC_act, odor_input, W):\n",
    "    return odor_input - np.matmul(W, GC_act)\n",
    "\n",
    "def get_loss(GC_act, odor_input, theta):\n",
    "    '''\n",
    "    Inputs:\n",
    "    1) gc activities\n",
    "    2) net mc activity, r_m\n",
    "    3) gc threshold, theta\n",
    "\n",
    "    Output: loss value\n",
    "    '''\n",
    "    MC_err = get_err(GC_act, odor_input, W)\n",
    "    loss = (1/2)*(sc.linalg.norm(MC_err, 2)**2) + theta*np.sum(GC_act)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_gradient(MC_err, theta, W):\n",
    "    '''\n",
    "    Inputs:\n",
    "    1) gc activities\n",
    "    2) net mc activity, r_m\n",
    "    3) MC-GC network\n",
    "\n",
    "    Function: gradient of the loss function w.r.t gc activations\n",
    "    '''\n",
    "  \n",
    "    grad = - np.matmul(W.T, MC_err) + theta\n",
    "\n",
    "    return grad\n",
    "\n",
    "def generalized_grad(GC_iter, grad, theta, t):\n",
    "    '''\n",
    "    Proximal gradient computation \n",
    "\n",
    "    Inputs: \n",
    "    1) iterate: current iterate\n",
    "    2) grad: gradient at current iterate\n",
    "    3) theta: gc threshold\n",
    "    4) t: steplength in [0,1]\n",
    "\n",
    "    Output: proximal gradient\n",
    "    '''\n",
    "    GC_aftergrad =  utils.project(GC_iter - t*grad, theta)\n",
    "\n",
    "    return GC_iter - GC_aftergrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_update(GC_act, W, projected_grad):\n",
    "    # ooops hessian is singular.. should have realized this :')\n",
    "    # d = - np.linalg.inv(np.matmul(W, np.transpose(W))) @ grad\n",
    "    pseudo_hess = np.linalg.pinv(np.matmul(W.T, W))\n",
    "    d = - pseudo_hess @ projected_grad\n",
    "    GC_act += d\n",
    "    return GC_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize:\n",
    "theta = .3\n",
    "GC_responpse = get_GCact(W, odors.T, theta)\n",
    "GC_act = GC_responpse[:,0]\n",
    "MC_err = odor1 - np.matmul(W, GC_act)\n",
    "loss = get_loss(GC_act,odor1, theta)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton update:\n",
    "newtons_losses = []\n",
    "for i in range(50):\n",
    "    MC_err = odor1 - np.matmul(W, GC_act)\n",
    "    grad = get_gradient(MC_err, theta, W)\n",
    "    # norm_grad = grad/sc.linalg.norm(grad,2)\n",
    "    projected_grad = generalized_grad(GC_act, grad, theta, t=1)\n",
    "    GC_act = newtons_update(GC_act, W, projected_grad)\n",
    "    loss = get_loss(GC_act, odor1, theta)\n",
    "    newtons_losses.append(loss)\n",
    "    if i%10 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(newtons_losses) # how can it go below???\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple line search: \n",
    "\n",
    "...it does not seem to be working very well with gradient projection; as most of the time projection reduced the gradent to a very tiny magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this seems to be very slow\n",
    "\n",
    "## Simple line search update\n",
    "\n",
    "alpha = .5 # \\in (0, 0.5)\n",
    "beta = .8 # \\in (0, 1)\n",
    "\n",
    "def line_search_update(grad, GC_act, odor_input, theta, curr_loss): \n",
    "    t = 1\n",
    "    gen_grad = generalized_grad(GC_act, grad, theta, t)\n",
    "    new_loss =  get_loss(GC_act - gen_grad, odor_input, theta)\n",
    "    # Armijo_bool = new_loss > curr_loss - alpha*t*(np.dot(grad, grad)) # I see grad*grad != generalized_grad* generalized_grad\n",
    "   \n",
    "    # a modification of the sufficient descent: \n",
    "    Armijo_bool = new_loss > curr_loss - alpha*t*(np.dot(gen_grad, gen_grad)) \n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    while Armijo_bool and count< 50:\n",
    "        t *= beta\n",
    "        curr_loss = new_loss\n",
    "        gen_grad = generalized_grad(GC_act, grad, theta, t)\n",
    "        new_loss =  get_loss(GC_act - gen_grad, odor_input, theta)\n",
    "        # Armijo_bool =  new_loss > curr_loss - alpha*t*(np.dot(grad, grad)) \n",
    "        Armijo_bool = new_loss > curr_loss - curr_loss - alpha*t*(np.dot(gen_grad, gen_grad)) \n",
    "        count += 1\n",
    "\n",
    "    GC_act -= gen_grad\n",
    "    \n",
    "    return GC_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize:\n",
    "theta = .3\n",
    "GC_responpse = get_GCact(W, odors.T, theta)\n",
    "GC_act = GC_responpse[:,0]\n",
    "MC_err = odor1 - np.matmul(W, GC_act)\n",
    "loss = get_loss(GC_act,odor1, theta)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_line_search_loss = []\n",
    "for i in range(2000):\n",
    "    MC_err = odor1 - np.matmul(W, GC_act)\n",
    "    grad = get_gradient(MC_err, theta, W)\n",
    "    GC_act = line_search_update(grad, GC_act, odor1, theta, loss)\n",
    "    loss = get_loss(GC_act, odor1, theta)\n",
    "    simp_line_search_loss.append(loss)\n",
    "    if i%10 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(simp_line_search_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal step size \n",
    "def line_search_PGD(GC_act, odor_input, theta, grad, loss_curr, beta):\n",
    "    '''\n",
    "    Backtracking line search for projected gradient descent\n",
    "\n",
    "    Inputs: \n",
    "    1) gc activities\n",
    "    2) net mc activities, r_m\n",
    "    3) theta: gc threshold\n",
    "    4) W: MC-GC network\n",
    "    5) grad: gradient at current iterate (current gc activities)\n",
    "    6) loss_curr: loss function value at current iterate\n",
    "    7) gamma: line search parameter \n",
    "\n",
    "    Output: steplength \n",
    "    '''\n",
    "    t = 1\n",
    "    iter = 50\n",
    "    for i in range(iter):\n",
    "        gen_grad = generalized_grad(GC_act, grad, theta, t)\n",
    "        # print('Norm of generalized gradient', sc.linalg.norm(gen_grad, 2))\n",
    "        new_iterate = GC_act - t*gen_grad\n",
    "        new_loss = get_loss(new_iterate, odor_input, theta)\n",
    "\n",
    "        ### I don't understand this...\n",
    "        quad_approx = loss_curr - (t*np.matmul(grad, np.transpose(gen_grad))) + (t/2)*(sc.linalg.norm(gen_grad, 2)**2)\n",
    "        \n",
    "        if new_loss < loss_curr and new_loss <= quad_approx:\n",
    "            break \n",
    "        else:\n",
    "            t = beta*t # backtrack till objective value at new point is smaller than a quadratic approximation\n",
    "            \n",
    "        ### I don't understand the quadratic approximation part why sometimes use grad, and sometimes use gen_grad?\n",
    "\n",
    "    return t if i < iter - 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize:\n",
    "theta = .3\n",
    "GC_responpse = get_GCact(W, odors.T, theta)\n",
    "GC_act = GC_responpse[:,0]\n",
    "MC_err = odor1 - np.matmul(W, GC_act)\n",
    "loss = get_loss(GC_act,odor1, theta)\n",
    "print(loss)\n",
    "simp_line_search_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "iters = 2    \n",
    "t = t if i < iters else 0\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jalaj_losses = []\n",
    "for i in range(2000):\n",
    "    MC_err = odor1 - np.matmul(W, GC_act)\n",
    "    grad = get_gradient(MC_err, theta, W)\n",
    "    norm_grad = grad/sc.linalg.norm(grad,2)  \n",
    "    steplength = line_search_PGD(GC_act, odor1, theta, norm_grad, loss, beta =.9)\n",
    "    if steplength > 0:\n",
    "        GC_act -= steplength*generalized_grad(GC_act, norm_grad, theta, steplength)            \n",
    "    loss = get_loss(GC_act, odor1, theta)\n",
    "    jalaj_losses.append(loss)\n",
    "    if i%10 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jalaj_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. track # of GC.\n",
    "\n",
    "--\n",
    "Outer loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = {'associate': 0.005, 'disassociate':0.005, 'forget':0.0005}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized updates: Plasticity\n",
    "def hebbian(GC_act, odor_input, etas, cap = True, cap_strength = 1):\n",
    "    for a in range(len(GC_act)):\n",
    "        for i in range(len(odor_input)):\n",
    "            if GC_act[a] > 0 and odor_input[i] > 0:\n",
    "                W[i,a] = min(W[i,a] + etas['associate']*odor_input[i]*GC_act[a], 1) \n",
    "            # can perhaps try not updating:\n",
    "            elif GC_act[a] > 0 and odor_input[i] < 0:\n",
    "                W[i,a] = max(W[i,a] + etas['disassociate']*odor_input[i]*GC_act[a], 0)\n",
    "            else:\n",
    "                W[i,a] = max(W[i,a] - etas['forget'], 0)\n",
    "        if cap:\n",
    "            if np.sum(W[:,a]) > cap_strength:\n",
    "                W[:,a] = (cap_strength*W[:,a])/np.sum(W[:,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniff_cycle(odor_input, GC_act, W):    \n",
    "    \n",
    "    for _ in range(1000):\n",
    "        MC_err = odor_input - np.matmul(W, GC_act)\n",
    "        grad = get_gradient(MC_err, theta, W)\n",
    "        norm_grad = grad/sc.linalg.norm(grad,2)  \n",
    "        steplength = line_search_PGD(GC_act, odor_input, theta, norm_grad, loss, beta =.9)\n",
    "        if steplength > 0:\n",
    "            GC_act -= steplength*generalized_grad(GC_act, norm_grad, theta, steplength)\n",
    "\n",
    "    return GC_act\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dictionary_learning():\n",
    "\n",
    "GC_responpse = get_GCact(W, odors.T, theta)\n",
    "GC_act_1 = GC_responpse[:,0]\n",
    "# MC_err_1 = odor1 - np.matmul(W, GC_act)\n",
    "\n",
    "GC_act_2= GC_responpse[:,1]\n",
    "\n",
    "\n",
    "for k in range(1000):\n",
    "    if k%2:\n",
    "        GC_act_1 = sniff_cycle(odor1, GC_act_1, W)\n",
    "        hebbian(GC_act_1, odor1, etas, cap = True, cap_strength = 1)\n",
    "\n",
    "    else: \n",
    "        GC_act_2 = sniff_cycle(odor2, GC_act_2, W)\n",
    "        hebbian(GC_act_2, odor2, etas, cap = True, cap_strength = 1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(W, cmap = 'flare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, p_conn = 10, 750, 0.05 # very sparse, as 0.01 gives us on average every GC connects to 1 MC.\n",
    "max_lim = 0.5 # for each entry Wnm the max connection strength\n",
    "\n",
    "W = utils.create_network(M, N, p_conn, max_lim)\n",
    "\n",
    "GC_responpse = get_GCact(W, odors.T, theta)\n",
    "GC_act_1 = GC_responpse[:,0]\n",
    "GC_act_2= GC_responpse[:,1]\n",
    "\n",
    "\n",
    "for k in range(1000):\n",
    "    if k%2:\n",
    "        GC_act_1 = sniff_cycle(odor1, GC_act_1, W)\n",
    "        hebbian(GC_act_1, odor1, etas, cap = True, cap_strength = 1)\n",
    "\n",
    "    else: \n",
    "        GC_act_2 = sniff_cycle(odor2, GC_act_2, W)\n",
    "        hebbian(GC_act_2, odor2, etas, cap = True, cap_strength = 1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(W, cmap = 'flare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try the other way round to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, p_conn = 10, 750, 0.05 # very sparse, as 0.01 gives us on average every GC connects to 1 MC.\n",
    "max_lim = 0.5 # for each entry Wnm the max connection strength\n",
    "\n",
    "W = utils.create_network(M, N, p_conn, max_lim)\n",
    "\n",
    "GC_responpse = get_GCact(W, odors.T, theta)\n",
    "GC_act_1 = GC_responpse[:,0]\n",
    "GC_act_2= GC_responpse[:,1]\n",
    "\n",
    "\n",
    "for k in range(1000):\n",
    "    if k%2 == 0:\n",
    "        GC_act_1 = sniff_cycle(odor1, GC_act_1, W)\n",
    "        hebbian(GC_act_1, odor1, etas, cap = True, cap_strength = 1)\n",
    "\n",
    "    else: \n",
    "        GC_act_2 = sniff_cycle(odor2, GC_act_2, W)\n",
    "        hebbian(GC_act_2, odor2, etas, cap = True, cap_strength = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(W, cmap = 'flare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we need to plot the stats?\n",
    "Where does the stochasticity coming from???!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
